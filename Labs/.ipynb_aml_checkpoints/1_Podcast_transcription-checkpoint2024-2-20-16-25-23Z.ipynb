{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "In this lab, you'll learn how to transform spoken words from podcasts into written text. To make this happen, we'll utilize the Open Whisper model. Whisper comes in handy for the following tasks:\n",
        "\n",
        "- Understanding spoken words in various languages.\n",
        "- Converting spoken words from podcasts into written text.\n",
        "- Translating speech from one language to another.\n",
        "- Determining the language being spoken\n",
        "\n",
        "# Objective\n",
        "- Learn to transcribe an audio file using Whisper model\n",
        "- Learn how to bring in and integrate library-based python models into the Copilot application\n",
        "\n",
        "### 1. Import required Python library: \n",
        "Start by including the necessary Python library to access the Whisper model.\n",
        "### 2. Transcribe the audio: \n",
        "Use Whisper to convert spoken words from audio into written text.\n",
        "### 3. Get the transcribed text: \n",
        "Retrieve the transcribed text as the final output."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import python libraries\n",
        "\n",
        "### Task 1: Import required Python libraries\n",
        "We will start by inserting the required libraries for the Lab. These libraries help support the manipulation and translation of the media we pass in, in our case an MP3 podcast.  \n",
        "\n",
        "Jn\n",
        "#### Task 1.1 Requirements.txt\n",
        "Let's start by adding our required libraries into `requirements.txt`\n",
        "\n",
        "### Task 1.2 Import Summary\n",
        "Lets learn about what our imports do:\n",
        "\n",
        "`import whisper:`\n",
        "\n",
        "The Whisper model is a speech to text model from OpenAI that you can use to transcribe audio files. The model is trained on a large dataset of English audio and text. The model is optimized for transcribing audio files that contain speech in English.\n",
        "\n",
        "`from pydub import AudioSegment and from pydub.silence import split_on_silence:`\n",
        "\n",
        "A library to manipulate audio with a simple and easy high-level interface. AudioSegment is used for representing audio segments, and split_on_silence is a function for splitting audio based on detected silence.\n",
        "\n",
        "`ort.set_default_logger_severity(3):`\n",
        "\n",
        "This line sets the default logging severity for ONNX Runtime to 3, which corresponds to a specific level of logging detail (like errors only).\n",
        "\n",
        "`imports from langchain.prompts:`\n",
        "\n",
        "LangChain is a library related to language processing or generation. The imports here are different types of prompt templates (AIMessagePromptTemplate, ChatPromptTemplate, etc.) used in the application, for AI interactions.\n",
        "\n",
        "`from langchain.schema import AIMessage, HumanMessage, SystemMessage:`\n",
        "\n",
        "This structures or generates a schema for messages in a chatbot or AI system. They define how different types of messages (AI-generated, human-generated, system messages) are structured."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -r requirements.txt"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import whisper\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence\n",
        "\n",
        "#from langchain.prompts import (AIMessagePromptTemplate, ChatPromptTemplate,\n",
        "#                               HumanMessagePromptTemplate, PromptTemplate,\n",
        "#                               SystemMessagePromptTemplate)\n",
        "#from langchain.schema import AIMessage, HumanMessage, SystemMessage"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710872797866
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get input file"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inputs about the podcast\n",
        "podcast_url = \"https://www.microsoft.com/behind-the-tech\"\n",
        "podcast_audio_file = \"../data/PodcastSnippet.mp3\""
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710872744396
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcribe the Audio with Azure OpenAI Whisper model\n",
        "\n",
        " In this task we will transcribe the audio to text using the Whisper model!"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "if load_dotenv():\n",
        "    print(\"Found Azure OpenAI API Base Endpoint: \" + os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\n",
        "else: \n",
        "    print(\"Azure OpenAI API Base Endpoint not found. Have you configured the .env file?\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found Azure OpenAI API Base Endpoint: https://swed-oai.openai.azure.com/\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710872311201
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create client to Azure OpenAI servoce\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
        "    api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
        ")\n",
        "\n",
        "model =  os.getenv(\"AZURE_OPENAI_WHISPER_MODEL\")\n",
        "print(\"Model: \", model)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model:  whisper\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710873552475
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Call Whisper model for transcription\n",
        "\n",
        "result = client.audio.transcriptions.create(\n",
        "            file=open(podcast_audio_file, \"rb\"),            \n",
        "            model=model\n",
        "            )\n",
        "\n",
        "transcript = result.text\n",
        "\n",
        "print(transcript)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Neil deGrasse Tyson is one of America's best-known astrophysicists and a beloved educator and advocate for the sciences. He has a great talent for presenting complex concepts in a clear and accessible manner. He's the head of the Hayden Planetarium and has been the director there since 1996. He's hosted numerous space-related TV and radio programs, published several books, and hosts the podcast StarTalk Radio. I am thrilled to have you on the podcast today, Neil. Well, thanks for having me. Why did it take you this long to invite me? I don't know. Shame, shame, shame on me. I'm not hidden, right? No, you aren't. And I will say, when I started this podcast and when I wrote my book and I started doing this very uncomfortable thing for me, which is trying to talk more about technology in the public, you were literally my role model. I said, Neil deGrasse Tyson does such a wonderful job communicating about the importance and the value of science to the public. We don't have people doing that about software and technology-related things. No, you don't. That's right. Very good point. I took you as a role model. Granted, I'm nowhere near as charming and as effective a communicator as you are, but I'm trying to do my best.\n"
        }
      ],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710873737779
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the file path\n",
        "file_path = \"transcript.txt\"\n",
        "\n",
        "# Write the content to the file\n",
        "with open(file_path, \"w\") as file:\n",
        "    file.write(transcript)"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710873771082
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcribe audio with Azure AI services,  Speech-to-Text model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcribe chunked file with whisper library"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunk up the audio file \n",
        "sound_file = AudioSegment.from_mp3(podcast_audio_file)\n",
        "audio_chunks = split_on_silence(sound_file, min_silence_len=1000, silence_thresh=-40 )\n",
        "count = len(audio_chunks)\n",
        "print(\"Audio split into \" + str(count) + \" audio chunks\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Audio split into 2 audio chunks\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710872811778
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "# Call Whisper to transcribe audio\n",
        "model = whisper.load_model(\"base\")\n",
        "transcript = \"\"\n",
        "for i, chunk in enumerate(audio_chunks):\n",
        "    # If you have a long audio file, you can enable this to only run for a subset of chunks\n",
        "    if i < 10 or i > count - 10:\n",
        "        out_file = \"chunk{0}.wav\".format(i)\n",
        "        print(\"Exporting\", out_file)\n",
        "        chunk.export(out_file, format=\"wav\")\n",
        "        result = model.transcribe(out_file)\n",
        "        transcriptChunk = result[\"text\"]\n",
        "        print(transcriptChunk)\n",
        "        \n",
        "        # Append transcript in memory if you have sufficient memory\n",
        "        transcript += \" \" + transcriptChunk\n",
        "\n",
        "print(\"Transcript: \\n\")\n",
        "print(transcript)\n",
        "print(\"\\n\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Exporting chunk0.wav\n Neil deGrasse Tyson is one of America's best known astrophysicist and a beloved educator and advocate for the sciences. He has a great talent for presenting complex concepts in a clear and accessible manner. He's the head of the Hayden Planetarium and has been the director there since 1996. He's hosted numerous space-related TV and radio programs, published several books, and hosts the podcast StarTalk Radio. I am thrilled to have you on the podcast today.\nExporting chunk1.wav\n Well, thanks for having me. So why do you take you this long to invite me? I just want to know. Shame, shame on me. I'm not hidden. Right. All right. And I will say, when I started this podcast, and when I wrote my book, and I started doing this very uncomfortable thing for me, which is trying to talk more about technology in the public, you were literally my role model. I said, Neil deGrasse Tyson does such a wonderful job communicating about the importance and the value of science to the public. We don't have people doing that about software and technology related things. No, you don't. That's right. Very good point. And I took you as a role model. And I granted, I'm nowhere near as charming as effective communicator as you are. But I'm trying to do my best.\nTranscript: \n\n  Neil deGrasse Tyson is one of America's best known astrophysicist and a beloved educator and advocate for the sciences. He has a great talent for presenting complex concepts in a clear and accessible manner. He's the head of the Hayden Planetarium and has been the director there since 1996. He's hosted numerous space-related TV and radio programs, published several books, and hosts the podcast StarTalk Radio. I am thrilled to have you on the podcast today.  Well, thanks for having me. So why do you take you this long to invite me? I just want to know. Shame, shame on me. I'm not hidden. Right. All right. And I will say, when I started this podcast, and when I wrote my book, and I started doing this very uncomfortable thing for me, which is trying to talk more about technology in the public, you were literally my role model. I said, Neil deGrasse Tyson does such a wonderful job communicating about the importance and the value of science to the public. We don't have people doing that about software and technology related things. No, you don't. That's right. Very good point. And I took you as a role model. And I granted, I'm nowhere near as charming as effective communicator as you are. But I'm trying to do my best.\n\n\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710872971832
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save transcription into file"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}