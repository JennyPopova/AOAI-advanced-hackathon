{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "In this lab, you'll learn how to transform spoken words from podcasts into written text. To make this happen, we'll utilize the Open Whisper model. Whisper comes in handy for the following tasks:\n",
        "\n",
        "- Understanding spoken words in various languages.\n",
        "- Converting spoken words from podcasts into written text.\n",
        "- Translating speech from one language to another.\n",
        "- Determining the language being spoken\n",
        "\n",
        "# Objective\n",
        "- Learn to transcribe an audio file using Whisper model\n",
        "- Learn how to bring in and integrate library-based python models into the Copilot application\n",
        "\n",
        "### 1. Import required Python library: \n",
        "Start by including the necessary Python library to access the Whisper model.\n",
        "### 2. Transcribe the audio: \n",
        "Use Whisper to convert spoken words from audio into written text.\n",
        "### 3. Get the transcribed text: \n",
        "Retrieve the transcribed text as the final output."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import python libraries\n",
        "\n",
        "### Task 1: Import required Python libraries\n",
        "We will start by inserting the required libraries for the Lab. These libraries help support the manipulation and translation of the media we pass in, in our case an MP3 podcast.  \n",
        "\n",
        "Jn\n",
        "#### Task 1.1 Requirements.txt\n",
        "Let's start by adding our required libraries into `requirements.txt`\n",
        "\n",
        "### Task 1.2 Import Summary\n",
        "Lets learn about what our imports do:\n",
        "\n",
        "`import whisper:`\n",
        "\n",
        "The Whisper model is a speech to text model from OpenAI that you can use to transcribe audio files. The model is trained on a large dataset of English audio and text. The model is optimized for transcribing audio files that contain speech in English.\n",
        "\n",
        "`from pydub import AudioSegment and from pydub.silence import split_on_silence:`\n",
        "\n",
        "A library to manipulate audio with a simple and easy high-level interface. AudioSegment is used for representing audio segments, and split_on_silence is a function for splitting audio based on detected silence.\n",
        "\n",
        "`ort.set_default_logger_severity(3):`\n",
        "\n",
        "This line sets the default logging severity for ONNX Runtime to 3, which corresponds to a specific level of logging detail (like errors only).\n",
        "\n",
        "`imports from langchain.prompts:`\n",
        "\n",
        "LangChain is a library related to language processing or generation. The imports here are different types of prompt templates (AIMessagePromptTemplate, ChatPromptTemplate, etc.) used in the application, for AI interactions.\n",
        "\n",
        "`from langchain.schema import AIMessage, HumanMessage, SystemMessage:`\n",
        "\n",
        "This structures or generates a schema for messages in a chatbot or AI system. They define how different types of messages (AI-generated, human-generated, system messages) are structured."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -r requirements.txt"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting python-dotenv==1.0.1\n  Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\nCollecting pydub==0.25.1\n  Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\nCollecting openai-whisper==20231117\n  Using cached openai-whisper-20231117.tar.gz (798 kB)\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\n\u001b[?25h    Preparing wheel metadata ... \u001b[?25l-\b \bdone\n\u001b[?25hCollecting openai==1.14.0\n  Using cached openai-1.14.0-py3-none-any.whl (257 kB)\nCollecting langchain==0.1.12\n  Using cached langchain-0.1.12-py3-none-any.whl (809 kB)\nRequirement already satisfied: numpy in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from openai-whisper==20231117->-r requirements.txt (line 3)) (1.21.6)\nRequirement already satisfied: tqdm in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from openai-whisper==20231117->-r requirements.txt (line 3)) (4.65.0)\nCollecting triton<3,>=2.0.0\n  Downloading triton-2.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n\u001b[K     |████████████████████████████████| 167.9 MB 19 kB/s s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numba in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from openai-whisper==20231117->-r requirements.txt (line 3)) (0.55.2)\nCollecting tiktoken\n  Downloading tiktoken-0.6.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n\u001b[K     |████████████████████████████████| 1.8 MB 84.0 MB/s eta 0:00:01\n\u001b[?25hCollecting more-itertools\n  Using cached more_itertools-10.2.0-py3-none-any.whl (57 kB)\nRequirement already satisfied: torch in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from openai-whisper==20231117->-r requirements.txt (line 3)) (1.12.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from openai==1.14.0->-r requirements.txt (line 4)) (1.10.8)\nRequirement already satisfied: anyio<5,>=3.5.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from openai==1.14.0->-r requirements.txt (line 4)) (3.6.2)\nRequirement already satisfied: distro<2,>=1.7.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from openai==1.14.0->-r requirements.txt (line 4)) (1.8.0)\nRequirement already satisfied: sniffio in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from openai==1.14.0->-r requirements.txt (line 4)) (1.3.0)\nCollecting httpx<1,>=0.23.0\n  Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\nCollecting typing-extensions<5,>=4.7\n  Using cached typing_extensions-4.10.0-py3-none-any.whl (33 kB)\nCollecting jsonpatch<2.0,>=1.33\n  Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from langchain==0.1.12->-r requirements.txt (line 5)) (3.8.4)\nRequirement already satisfied: requests<3,>=2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from langchain==0.1.12->-r requirements.txt (line 5)) (2.31.0)\nCollecting langchain-community<0.1,>=0.0.28\n  Using cached langchain_community-0.0.28-py3-none-any.whl (1.8 MB)\nCollecting langchain-text-splitters<0.1,>=0.0.1\n  Using cached langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\nCollecting langchain-core<0.2.0,>=0.1.31\n  Using cached langchain_core-0.1.32-py3-none-any.whl (260 kB)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0; python_version < \"3.11\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from langchain==0.1.12->-r requirements.txt (line 5)) (4.0.2)\nCollecting langsmith<0.2.0,>=0.1.17\n  Downloading langsmith-0.1.29-py3-none-any.whl (70 kB)\n\u001b[K     |████████████████████████████████| 70 kB 8.3 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from langchain==0.1.12->-r requirements.txt (line 5)) (6.0)\nCollecting SQLAlchemy<3,>=1.4\n  Downloading SQLAlchemy-2.0.28-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[K     |████████████████████████████████| 3.1 MB 70.1 MB/s eta 0:00:01\n\u001b[?25hCollecting dataclasses-json<0.7,>=0.5.7\n  Using cached dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from langchain==0.1.12->-r requirements.txt (line 5)) (8.2.2)\nRequirement already satisfied: filelock in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from triton<3,>=2.0.0->openai-whisper==20231117->-r requirements.txt (line 3)) (3.12.0)\nRequirement already satisfied: setuptools in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from numba->openai-whisper==20231117->-r requirements.txt (line 3)) (65.6.3)\nRequirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from numba->openai-whisper==20231117->-r requirements.txt (line 3)) (0.38.1)\nRequirement already satisfied: regex>=2022.1.18 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from tiktoken->openai-whisper==20231117->-r requirements.txt (line 3)) (2023.5.5)\nRequirement already satisfied: idna>=2.8 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from anyio<5,>=3.5.0->openai==1.14.0->-r requirements.txt (line 4)) (3.4)\nCollecting httpcore==1.*\n  Using cached httpcore-1.0.4-py3-none-any.whl (77 kB)\nRequirement already satisfied: certifi in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from httpx<1,>=0.23.0->openai==1.14.0->-r requirements.txt (line 4)) (2022.9.24)\nRequirement already satisfied: jsonpointer>=1.9 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.12->-r requirements.txt (line 5)) (2.4)\nRequirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12->-r requirements.txt (line 5)) (6.0.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12->-r requirements.txt (line 5)) (1.3.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12->-r requirements.txt (line 5)) (1.3.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12->-r requirements.txt (line 5)) (1.9.2)\nRequirement already satisfied: attrs>=17.3.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12->-r requirements.txt (line 5)) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12->-r requirements.txt (line 5)) (3.1.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests<3,>=2->langchain==0.1.12->-r requirements.txt (line 5)) (1.26.16)\nCollecting packaging<24.0,>=23.2\n  Using cached packaging-23.2-py3-none-any.whl (53 kB)\nCollecting orjson<4.0.0,>=3.9.14\n  Downloading orjson-3.9.15-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001b[K     |████████████████████████████████| 138 kB 86.6 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: greenlet!=0.4.17; platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\"))))) in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.12->-r requirements.txt (line 5)) (2.0.2)\nCollecting marshmallow<4.0.0,>=3.18.0\n  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n\u001b[K     |████████████████████████████████| 49 kB 4.9 MB/s  eta 0:00:01\n\u001b[?25hCollecting typing-inspect<1,>=0.4.0\n  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\nRequirement already satisfied: h11<0.15,>=0.13 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.14.0->-r requirements.txt (line 4)) (0.14.0)\nCollecting mypy-extensions>=0.3.0\n  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\nBuilding wheels for collected packages: openai-whisper\n  Building wheel for openai-whisper (PEP 517) ... \u001b[?25l-\b \b\\\b \bdone\n\u001b[?25h  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801358 sha256=452a18da3f31dae587369bec2af48be844e76cc9d5edbe6bcb65009363dedc7f\n  Stored in directory: /home/azureuser/.cache/pip/wheels/d2/33/5e/ab7fe45178ca9489707f18a89fd9a22611b656edf804b3cf53\nSuccessfully built openai-whisper\n\u001b[31mERROR: tensorflow 2.11.0 has requirement protobuf<3.20,>=3.9.2, but you'll have protobuf 3.20.3 which is incompatible.\u001b[0m\n\u001b[31mERROR: tensorboardx 2.6.1 has requirement protobuf>=4.22.3, but you'll have protobuf 3.20.3 which is incompatible.\u001b[0m\n\u001b[31mERROR: scikit-image 0.21.0 has requirement networkx>=2.8, but you'll have networkx 2.5 which is incompatible.\u001b[0m\n\u001b[31mERROR: scikit-image 0.21.0 has requirement scipy>=1.8, but you'll have scipy 1.5.3 which is incompatible.\u001b[0m\n\u001b[31mERROR: nbconvert 7.4.0 has requirement jinja2>=3.0, but you'll have jinja2 2.11.2 which is incompatible.\u001b[0m\n\u001b[31mERROR: jupyterlab 3.2.4 has requirement jupyter-server~=1.4, but you'll have jupyter-server 2.5.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: jupyterlab 3.2.4 has requirement nbclassic~=0.2, but you'll have nbclassic 1.0.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: jupyterlab-server 2.23.0 has requirement jinja2>=3.0.3, but you'll have jinja2 2.11.2 which is incompatible.\u001b[0m\n\u001b[31mERROR: fastparquet 2023.4.0 has requirement pandas>=1.5.0, but you'll have pandas 1.1.5 which is incompatible.\u001b[0m\n\u001b[31mERROR: datasets 2.3.2 has requirement dill<0.3.6, but you'll have dill 0.3.6 which is incompatible.\u001b[0m\n\u001b[31mERROR: dask-sql 2023.6.0 has requirement pandas>=1.4.0, but you'll have pandas 1.1.5 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-inference-server-http 0.8.4 has requirement flask<2.3.0, but you'll have flask 2.3.2 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-core 1.51.0 has requirement packaging<=23.0,>=20.0, but you'll have packaging 23.2 which is incompatible.\u001b[0m\n\u001b[31mERROR: azure-cli 2.49.0 has requirement azure-keyvault-keys==4.8.0b2, but you'll have azure-keyvault-keys 4.8.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azure-cli 2.49.0 has requirement azure-mgmt-keyvault==10.2.0, but you'll have azure-mgmt-keyvault 10.2.1 which is incompatible.\u001b[0m\n\u001b[31mERROR: azure-cli 2.49.0 has requirement azure-mgmt-resource==22.0.0, but you'll have azure-mgmt-resource 21.1.0b1 which is incompatible.\u001b[0m\n\u001b[31mERROR: azure-cli-core 2.49.0 has requirement msal[broker]==1.20.0, but you'll have msal 1.22.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: autokeras 1.0.16 has requirement tensorflow<=2.5.0,>=2.3.0, but you'll have tensorflow 2.11.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: arviz 0.11.2 has requirement typing-extensions<4,>=3.7.4.3, but you'll have typing-extensions 4.10.0 which is incompatible.\u001b[0m\nInstalling collected packages: python-dotenv, pydub, triton, tiktoken, more-itertools, openai-whisper, httpcore, httpx, typing-extensions, openai, jsonpatch, packaging, orjson, langsmith, langchain-core, SQLAlchemy, marshmallow, mypy-extensions, typing-inspect, dataclasses-json, langchain-community, langchain-text-splitters, langchain\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing-extensions 4.6.0\n    Uninstalling typing-extensions-4.6.0:\n      Successfully uninstalled typing-extensions-4.6.0\n  Attempting uninstall: packaging\n    Found existing installation: packaging 23.0\n    Uninstalling packaging-23.0:\n      Successfully uninstalled packaging-23.0\nSuccessfully installed SQLAlchemy-2.0.28 dataclasses-json-0.6.4 httpcore-1.0.4 httpx-0.27.0 jsonpatch-1.33 langchain-0.1.12 langchain-community-0.0.28 langchain-core-0.1.32 langchain-text-splitters-0.0.1 langsmith-0.1.29 marshmallow-3.21.1 more-itertools-10.2.0 mypy-extensions-1.0.0 openai-1.14.0 openai-whisper-20231117 orjson-3.9.15 packaging-23.2 pydub-0.25.1 python-dotenv-1.0.1 tiktoken-0.6.0 triton-2.2.0 typing-extensions-4.10.0 typing-inspect-0.9.0\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence\n",
        "\n",
        "from langchain.prompts import (AIMessagePromptTemplate, ChatPromptTemplate,\n",
        "                               HumanMessagePromptTemplate, PromptTemplate,\n",
        "                               SystemMessagePromptTemplate)\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710869072886
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Get input file and chunk it into pieces"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inputs about the podcast\n",
        "podcast_url = \"https://www.microsoft.com/behind-the-tech\"\n",
        "podcast_audio_file = \"../data/PodcastSnippet.mp3\""
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710869601324
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunk up the audio file \n",
        "sound_file = AudioSegment.from_mp3(podcast_audio_file)\n",
        "audio_chunks = split_on_silence(sound_file, min_silence_len=1000, silence_thresh=-40 )\n",
        "count = len(audio_chunks)\n",
        "print(\"Audio split into \" + str(count) + \" audio chunks\")"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AudioSegment' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Chunk up the audio file \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sound_file \u001b[38;5;241m=\u001b[39m \u001b[43mAudioSegment\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_mp3(podcast_audio_file)\n\u001b[1;32m      3\u001b[0m audio_chunks \u001b[38;5;241m=\u001b[39m split_on_silence(sound_file, min_silence_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, silence_thresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m40\u001b[39m )\n\u001b[1;32m      4\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(audio_chunks)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AudioSegment' is not defined"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710869614892
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Transcribe the Audio\n",
        "\n",
        " In this task we will transcribe the audio to text using the Whisper model!"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import time\n",
        "import os\n",
        "\n",
        "openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
        "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")  # your endpoint should look like the following https://YOUR_RESOURCE_NAME.openai.azure.com/\n",
        "openai.api_type = \"azure\"\n",
        "openai.api_version = \"2024-02-01\"\n",
        "\n",
        "model_name = \"whisper\"\n",
        "deployment_id = \"YOUR-DEPLOYMENT-NAME-HERE\" #This will correspond to the custom name you chose for your deployment when you deployed a model.\"\n",
        "audio_language=\"en\"\n",
        "\n",
        "audio_test_file = podcast_audio_file\n",
        "\n",
        "result = openai.Audio.transcribe(\n",
        "            file=open(audio_test_file, \"rb\"),            \n",
        "            model=model_name,\n",
        "            deployment_id=deployment_id\n",
        "        )\n",
        "\n",
        "print(result)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "APIRemovedInV1",
          "evalue": "\n\nYou tried to access openai.Audio, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m audio_language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m audio_test_file \u001b[38;5;241m=\u001b[39m podcast_audio_file\n\u001b[0;32m---> 16\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maudio_test_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdeployment_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeployment_id\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Audio, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710869622322
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call Whisper to transcribe audio\n",
        "model = whisper.load_model(\"base\")\n",
        "transcript = \"\"\n",
        "for i, chunk in enumerate(audio_chunks):\n",
        "    # If you have a long audio file, you can enable this to only run for a subset of chunks\n",
        "    if i < 10 or i > count - 10:\n",
        "        out_file = \"chunk{0}.wav\".format(i)\n",
        "        print(\"Exporting\", out_file)\n",
        "        chunk.export(out_file, format=\"wav\")\n",
        "        result = model.transcribe(out_file)\n",
        "        transcriptChunk = result[\"text\"]\n",
        "        print(transcriptChunk)\n",
        "        \n",
        "        # Append transcript in memory if you have sufficient memory\n",
        "        transcript += \" \" + transcriptChunk\n",
        "\n",
        "print(\"Transcript: \\n\")\n",
        "print(transcript)\n",
        "print(\"\\n\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Exporting chunk0.wav\n Neil deGrasse Tyson is one of America's best known astrophysicist and a beloved educator and advocate for the sciences. He has a great talent for presenting complex concepts in a clear and accessible manner. He's the head of the Hayden Planetarium and has been the director there since 1996. He's hosted numerous space-related TV and radio programs, published several books, and hosts the podcast StarTalk Radio. I am thrilled to have you on the podcast today.\nExporting chunk1.wav\n Well, thanks for having me. So why do you take you this long to invite me? I just want to know. Shame, shame on me. I'm not hidden. Right. All right. And I will say, when I started this podcast, and when I wrote my book, and I started doing this very uncomfortable thing for me, which is trying to talk more about technology in the public, you were literally my role model. I said, Neil deGrasse Tyson does such a wonderful job communicating about the importance and the value of science to the public. We don't have people doing that about software and technology related things. No, you don't. That's right. Very good point. And I took you as a role model. And I granted, I'm nowhere near as charming as effective communicator as you are. But I'm trying to do my best.\nTranscript: \n\n  Neil deGrasse Tyson is one of America's best known astrophysicist and a beloved educator and advocate for the sciences. He has a great talent for presenting complex concepts in a clear and accessible manner. He's the head of the Hayden Planetarium and has been the director there since 1996. He's hosted numerous space-related TV and radio programs, published several books, and hosts the podcast StarTalk Radio. I am thrilled to have you on the podcast today.  Well, thanks for having me. So why do you take you this long to invite me? I just want to know. Shame, shame on me. I'm not hidden. Right. All right. And I will say, when I started this podcast, and when I wrote my book, and I started doing this very uncomfortable thing for me, which is trying to talk more about technology in the public, you were literally my role model. I said, Neil deGrasse Tyson does such a wonderful job communicating about the importance and the value of science to the public. We don't have people doing that about software and technology related things. No, you don't. That's right. Very good point. And I took you as a role model. And I granted, I'm nowhere near as charming as effective communicator as you are. But I'm trying to do my best.\n\n\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710867719385
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Save transcription into file"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the file path\n",
        "file_path = \"transcript.txt\"\n",
        "\n",
        "# Write the content to the file\n",
        "with open(file_path, \"w\") as file:\n",
        "    file.write(transcript)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710518161405
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}