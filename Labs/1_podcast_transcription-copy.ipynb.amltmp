{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "In this lab, you'll learn how to transform spoken words from podcasts into written text. To make this happen, we'll utilize the Open Whisper model. Whisper comes in handy for the following tasks:\n",
        "\n",
        "- Understanding spoken words in various languages.\n",
        "- Converting spoken words from podcasts into written text.\n",
        "- Translating speech from one language to another.\n",
        "- Determining the language being spoken\n",
        "\n",
        "# Objective\n",
        "- Learn to transcribe an audio file using Whisper model\n",
        "- Learn how to bring in and integrate library-based python models into the Copilot application\n",
        "\n",
        "### 1. Import required Python library: \n",
        "Start by including the necessary Python library to access the Whisper model.\n",
        "### 2. Transcribe the audio: \n",
        "Use Whisper to convert spoken words from audio into written text.\n",
        "### 3. Get the transcribed text: \n",
        "Retrieve the transcribed text as the final output."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import python libraries\n",
        "\n",
        "### Task 1: Import required Python libraries\n",
        "We will start by inserting the required libraries for the Lab. These libraries help support the manipulation and translation of the media we pass in, in our case an MP3 podcast.  \n",
        "\n",
        "Jn\n",
        "#### Task 1.1 Requirements.txt\n",
        "Let's start by adding our required libraries into `requirements.txt`\n",
        "\n",
        "### Task 1.2 Import Summary\n",
        "Lets learn about what our imports do:\n",
        "\n",
        "`import whisper:`\n",
        "\n",
        "The Whisper model is a speech to text model from OpenAI that you can use to transcribe audio files. The model is trained on a large dataset of English audio and text. The model is optimized for transcribing audio files that contain speech in English.\n",
        "\n",
        "`from pydub import AudioSegment and from pydub.silence import split_on_silence:`\n",
        "\n",
        "A library to manipulate audio with a simple and easy high-level interface. AudioSegment is used for representing audio segments, and split_on_silence is a function for splitting audio based on detected silence.\n",
        "\n",
        "`ort.set_default_logger_severity(3):`\n",
        "\n",
        "This line sets the default logging severity for ONNX Runtime to 3, which corresponds to a specific level of logging detail (like errors only).\n",
        "\n",
        "`imports from langchain.prompts:`\n",
        "\n",
        "LangChain is a library related to language processing or generation. The imports here are different types of prompt templates (AIMessagePromptTemplate, ChatPromptTemplate, etc.) used in the application, for AI interactions.\n",
        "\n",
        "`from langchain.schema import AIMessage, HumanMessage, SystemMessage:`\n",
        "\n",
        "This structures or generates a schema for messages in a chatbot or AI system. They define how different types of messages (AI-generated, human-generated, system messages) are structured."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -r requirements.txt"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence\n",
        "\n",
        "from langchain.prompts import (AIMessagePromptTemplate, ChatPromptTemplate,\n",
        "                               HumanMessagePromptTemplate, PromptTemplate,\n",
        "                               SystemMessagePromptTemplate)\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710869072886
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Get input file and chunk it into pieces"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inputs about the podcast\n",
        "podcast_url = \"https://www.microsoft.com/behind-the-tech\"\n",
        "podcast_audio_file = \"../data/PodcastSnippet.mp3\""
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710870316264
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunk up the audio file \n",
        "sound_file = AudioSegment.from_mp3(podcast_audio_file)\n",
        "audio_chunks = split_on_silence(sound_file, min_silence_len=1000, silence_thresh=-40 )\n",
        "count = len(audio_chunks)\n",
        "print(\"Audio split into \" + str(count) + \" audio chunks\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710869614892
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "if load_dotenv():\n",
        "    print(\"Found Azure OpenAI API Base Endpoint: \" + os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\n",
        "else: \n",
        "    print(\"Azure OpenAI API Base Endpoint not found. Have you configured the .env file?\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found Azure OpenAI API Base Endpoint: https://swed-oai.openai.azure.com/\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710870271979
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import AzureOpenAI\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
        "    api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
        ")\n",
        "\n",
        "model =  os.getenv(\"AZURE_OPENAI_WHISPER_MODEL\")\n",
        "print(model)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "whisper\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710870703222
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Transcribe the Audio\n",
        "\n",
        " In this task we will transcribe the audio to text using the Whisper model!"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import time\n",
        "import os\n",
        "\n",
        "openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
        "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")  # your endpoint should look like the following https://YOUR_RESOURCE_NAME.openai.azure.com/\n",
        "openai.api_type = \"azure\"\n",
        "openai.api_version = \"2024-02-01\"\n",
        "\n",
        "model_name = \"whisper\"\n",
        "deployment_id = model #This will correspond to the custom name you chose for your deployment when you deployed a model.\"\n",
        "audio_language=\"en\"\n",
        "\n",
        "audio_test_file = podcast_audio_file\n",
        "\n",
        "result = client.audio.transcribe(\n",
        "            file=open(audio_test_file, \"rb\"),            \n",
        "            model=model_name,\n",
        "            deployment_id=deployment_id\n",
        "        )\n",
        "\n",
        "print(result)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Audio' object has no attribute 'transcribe'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m audio_language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m audio_test_file \u001b[38;5;241m=\u001b[39m podcast_audio_file\n\u001b[0;32m---> 16\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m(\n\u001b[1;32m     17\u001b[0m             file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mopen\u001b[39m(audio_test_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m),            \n\u001b[1;32m     18\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[1;32m     19\u001b[0m             deployment_id\u001b[38;5;241m=\u001b[39mdeployment_id\n\u001b[1;32m     20\u001b[0m         )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Audio' object has no attribute 'transcribe'"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710870734958
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call Whisper to transcribe audio\n",
        "model = whisper.load_model(\"base\")\n",
        "transcript = \"\"\n",
        "for i, chunk in enumerate(audio_chunks):\n",
        "    # If you have a long audio file, you can enable this to only run for a subset of chunks\n",
        "    if i < 10 or i > count - 10:\n",
        "        out_file = \"chunk{0}.wav\".format(i)\n",
        "        print(\"Exporting\", out_file)\n",
        "        chunk.export(out_file, format=\"wav\")\n",
        "        result = model.transcribe(out_file)\n",
        "        transcriptChunk = result[\"text\"]\n",
        "        print(transcriptChunk)\n",
        "        \n",
        "        # Append transcript in memory if you have sufficient memory\n",
        "        transcript += \" \" + transcriptChunk\n",
        "\n",
        "print(\"Transcript: \\n\")\n",
        "print(transcript)\n",
        "print(\"\\n\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710867719385
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Save transcription into file"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the file path\n",
        "file_path = \"transcript.txt\"\n",
        "\n",
        "# Write the content to the file\n",
        "with open(file_path, \"w\") as file:\n",
        "    file.write(transcript)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710518161405
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}