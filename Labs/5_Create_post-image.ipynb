{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Overview\n",
        "In this lab we will call **DALL-E** model API from Lang chain application for getting social media pimage illustrationost by sending social media post as an input.\n",
        "\n",
        "**DALL-E** is a powerful AI model that can generate and edit images based on natural language prompts. **Image client** is a Python library that allows you to interact with DALL-E and other image models using a simple and intuitive interface. GPT-4 is a state-of-the-art language model that can understand and generate natural language or code. By combining these three components, you can create amazing applications that use natural language to generate and manipulate images. For example, you can use GPT-4 to generate a prompt for DALL-E, such as “a cat wearing a hat”, and then use image client to send the prompt to DALL-E and receive an image as a response. You can also use image client to edit the image, such as changing the color or shape of the hat, and then use GPT-4 to generate a description of the edited image. \n",
        "\n",
        "### Steps:\n",
        "\n",
        "1. Generate a prompt for DALL-E using GPT model\n",
        "2. Use image client to send the prompt to DALL-E and receive an image as a response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Import python libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710960974598
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import AzureChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import (\n",
        "    PromptTemplate,\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Get social media post from previous Lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710960976039
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Open the file\n",
        "with open('social_media_post.txt', 'r') as f:\n",
        "    social_media_post = f.read()\n",
        "\n",
        "# Print the content\n",
        "print(social_media_post)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Get Environment variables\n",
        "The provided code is importing the `load_dotenv` function from the `dotenv` module and using it to load environment variables from a `.env`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710960995450
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "if load_dotenv():\n",
        "    print(\"Found Azure OpenAI API Base Endpoint: \" + os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\n",
        "else: \n",
        "    print(\"Azure OpenAI API Base Endpoint not found. Have you configured the .env file?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710960999542
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
        "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
        "\n",
        "model =  os.getenv(\"AZURE_OPENAI_CHAT_MODEL\")\n",
        "print(model)\n",
        "\n",
        "dalle_model = os.getenv(\"AZURE_OPENAI_DALLE_MODEL\")\n",
        "print(dalle_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Use GPT model to generate prompt for DALL-E model\n",
        "This Python code snippet is used to generate a DALL-E prompt using the Azure OpenAI Service with the GPT-4 model. The DALL-E prompt is generated based on the social media post created in the previous steps.\n",
        "\n",
        "- A `system_template` string is defined that describes the task of the AI model.\n",
        "- A `user_prompt` is created using the `PromptTemplate` class. This prompt includes a template for the DALL-E prompt and a placeholder for the social media post.\n",
        "- The `HumanMessagePromptTemplate` class is used to create a human message prompt from the `user_prompt`.\n",
        "- An instance of `AzureChatOpenAI` is created, which is a wrapper for the Azure OpenAI Service. The API version and the model name are passed as arguments.\n",
        "- The `AzureChatOpenAI` instance is called with the `dalle_messages` as an argument. This sends a request to the Azure OpenAI Service to generate a DALL-E prompt based on the `dalle_messages`.\n",
        "- An instance of `LLMChain` is created with the `AzureChatOpenAI` instance, the chat prompt, and the output key as arguments. This object represents a chain of language model calls.\n",
        "- The content of the response from the Azure OpenAI Service is stored in the `dalle_prompt` variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710961008437
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Use GPT-4 to generate a DALL-E prompt\n",
        "system_template=\"You are a helpful large language model that generates DALL-E prompts, that when given to the DALL-E model can generate beautiful high-quality images to use in social media posts about a podcast on technology.  Good DALL-E prompts will contain mention of related objects, and will not contain people or words.  Good DALL-E prompts should include a reference to podcasting along with items from the domain of the podcast guest.\\n\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "\n",
        "user_prompt=PromptTemplate(\n",
        "    template=\"Create a DALL-E prompt to create an image to post along with this social media text: {social_media_copy}\",\n",
        "    input_variables=[\"social_media_copy\"],\n",
        ")\n",
        "human_message_prompt = HumanMessagePromptTemplate(prompt=user_prompt)\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "# Get formatted messages for the chat completion\n",
        "dalle_messages = chat_prompt.format_prompt(social_media_copy={social_media_post}).to_messages()\n",
        "\n",
        "# Call Azure OpenAI Service to get a DALL-E prompt \n",
        "print(\"Calling GPT-4 model on Azure OpenAI Service to get a DALL-E prompt...\\n\")\n",
        "gpt4 = AzureChatOpenAI(\n",
        "    openai_api_version=api_version,\n",
        "    deployment_name=model,\n",
        "    openai_api_type = \"azure\",\n",
        ")\n",
        "\n",
        "output = gpt4(dalle_messages)\n",
        "dalle_prompt = output.content\n",
        "\n",
        "dalle_prompt_chain = LLMChain(llm=gpt4, prompt=chat_prompt, output_key=\"dalle_prompt\")\n",
        "\n",
        "print(\"DALL-E Prompt:\")\n",
        "print(dalle_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Save DALL-E prompt to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710960231518
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Specify the file path\n",
        "file_path = \"dalle_prompt.txt\"\n",
        "\n",
        "# Write the content to the file\n",
        "with open(file_path, \"w\") as file:\n",
        "    file.write(dalle_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Alter DALL-E prompt\n",
        "Append \"high-quality digital art\" to the generated DALL-E prompt as a guidance for the model how to create an image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710961018860
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Append \"high-quality digital art\" to the generated DALL-E prompt\n",
        "dalle_prompt = dalle_prompt + \", high-quality digital art\"\n",
        "print(\"dalleprompt: \",dalle_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Make a call to DALL-E model on the Azure OpenAI Service to generate an image \n",
        "This code is creating AzureOpenAI client and makes actual call to **DALL-E** model to create an image\n",
        "\n",
        "*You can experiment further and try your own `dalle_prompt`*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from openai import AzureOpenAI\n",
        "import requests\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    api_version=\"2024-02-01\",  \n",
        "    api_key=api_key,  \n",
        "    azure_endpoint=azure_endpoint\n",
        ")\n",
        "\n",
        "result = client.images.generate(\n",
        "    model=dalle_model, # the name of your DALL-E 3 deployment\n",
        "    prompt=dalle_prompt,\n",
        "    n=1\n",
        ")\n",
        "\n",
        "json_response = json.loads(result.model_dump_json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Get the image and display it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710961110078
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Initialize the image path (note the filetype should be png)\n",
        "image_path = 'postImage.png'\n",
        "\n",
        "# Retrieve the generated image\n",
        "image_url = json_response[\"data\"][0][\"url\"]  # extract image URL from response\n",
        "generated_image = requests.get(image_url).content  # download the image\n",
        "with open(image_path, \"wb\") as image_file:\n",
        "    image_file.write(generated_image)\n",
        "\n",
        "# Display the image in the default image viewer\n",
        "image = Image.open(image_path)\n",
        "image"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
