{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Overview\n",
        "\n",
        "This tutorial will guide you through the process of converting spoken language from podcasts into written text using the Open Whisper model. The Whisper model is useful for:\n",
        "\n",
        "- Interpreting spoken words in multiple languages.\n",
        "- Transcribing spoken words from podcasts into text.\n",
        "- Translating spoken language from one to another.\n",
        "- Identifying the language being spoken.\n",
        "\n",
        "### Goals\n",
        "\n",
        "- Understand how to transcribe an audio file using the Whisper model.\n",
        "- Learn to incorporate and integrate Python library-based models into the Copilot application.\n",
        "\n",
        "### Steps\n",
        "\n",
        "1. **Include Necessary Python Library:** Begin by importing the required Python library to utilize the Whisper model.\n",
        "\n",
        "2. **Audio Transcription:** Employ Whisper to transcribe spoken words from audio into text.\n",
        "\n",
        "3. **Retrieve Transcribed Text:** Obtain the transcribed text as the final output and store it in a file for subsequent processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Requirements.txt\n",
        "Before running the code, make sure that you have installed the necessary Python libraries by executing `pip install -r requirements.txt` in the Terminal.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Get input file\n",
        "This Python code snippet is used to read podcast audio file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710952437542
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Inputs about the podcast\n",
        "podcast_url = \"https://www.microsoft.com/behind-the-tech\"\n",
        "podcast_audio_file = \"../data/PodcastSnippet.mp3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1. Transcribe the Audio with Azure OpenAI Whisper model\n",
        "\n",
        "In this task, we will transcribe the audio to text using the Whisper model!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Connect to Azure OpenAI service"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The provided code is importing the `load_dotenv` function from the `dotenv` module and using it to load environment variables from a `.env`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710872311201
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "if load_dotenv():\n",
        "    print(\"Found Azure OpenAI API Base Endpoint: \" + os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\n",
        "else: \n",
        "    print(\"Azure OpenAI API Base Endpoint not found. Have you configured the .env file?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This Python code snippet is used to create a client for the Azure OpenAI service. The client is used to interact with the OpenAI API, which provides various AI capabilities such as natural language processing, machine learning models, and more.\n",
        "\n",
        "The `AzureOpenAI` class from the `openai` module is imported at the beginning. This class is used to create a client that can interact with the Azure OpenAI service.\n",
        "\n",
        "An instance of the `AzureOpenAI` class is created and assigned to the variable `client`. The constructor for this class takes three arguments: `azure_endpoint`, `api_key`, and `api_version`. These values are fetched from the environment variables using the `os.getenv` function.\n",
        "\n",
        "- `azure_endpoint` is the URL of the Azure OpenAI service that the client will interact with.\n",
        "- `api_key` is the secret key used for authentication with the Azure OpenAI service.\n",
        "- `api_version` is the version of the Azure OpenAI API that the client will use.\n",
        "\n",
        "Next, the Whisper model to be used for transcription is fetched from the environment variables and stored in the `model` variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710873552475
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#create client to Azure OpenAI servoce\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
        "    api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
        ")\n",
        "\n",
        "model =  os.getenv(\"AZURE_OPENAI_WHISPER_MODEL\")\n",
        "print(\"Model: \", model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Send request to Whisper model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This Python code snippet is used to transcribe audio from a file using the Whisper model from the Azure OpenAI service.\n",
        "\n",
        "The `client.audio.transcriptions.create` method is called to create a new transcription. This method takes two arguments: the audio file to transcribe and the model to use for transcription. The audio file is opened in binary mode for reading and passed to the method. The model to use for transcription is specified by the `model` variable, which is expected to be 'whisper'.\n",
        "\n",
        "The `create` method returns a result object, which contains the transcription result. The transcribed text can be accessed through the `text` attribute of the result object. This transcribed text is stored in the `transcript` variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710873737779
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#Call Whisper model for transcription\n",
        "\n",
        "result = client.audio.transcriptions.create(\n",
        "            file=open(podcast_audio_file, \"rb\"),            \n",
        "            model=model\n",
        "            )\n",
        "\n",
        "transcript = result.text\n",
        "\n",
        "print(transcript)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Save transcription into file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This Python code snippet is used to write the transcribed text to `\"transcript.txt\"` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710873771082
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Specify the file path\n",
        "file_path = \"transcript.txt\"\n",
        "\n",
        "# Write the content to the file\n",
        "with open(file_path, \"w\") as file:\n",
        "    file.write(transcript)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Transcribe chunked file with whisper library\n",
        "In the subsequent section of this Lab, we'll explore an alternative method for transcribing audio. If you're dealing with a lengthy audio file, it might be beneficial to first divide it into smaller segments and then process each segment individually.\n",
        "\n",
        "Before proceeding, ensure that the `ffmpeg` library is installed on your system. You can do this by executing the following commands in the terminal:\n",
        "```sh\n",
        "sudo add-apt-repository ppa:mc3man/trusty-media\n",
        "sudo apt-get update\n",
        "sudo apt-get install ffmpeg\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Import required Python libraries\n",
        "We will start by inserting the required libraries. These libraries help support the manipulation and translation of the media we pass in, in our case an MP3 podcast.  \n",
        "\n",
        "Lets learn about what our imports do:\n",
        "\n",
        "`import whisper:`\n",
        "\n",
        "The Whisper model is a speech to text model from OpenAI that you can use to transcribe audio files. The model is trained on a large dataset of English audio and text. The model is optimized for transcribing audio files that contain speech in English.\n",
        "\n",
        "`from pydub import AudioSegment and from pydub.silence import split_on_silence:`\n",
        "\n",
        "A library to manipulate audio with a simple and easy high-level interface. AudioSegment is used for representing audio segments, and split_on_silence is a function for splitting audio based on detected silence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710952428354
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chunk up the audio file \n",
        "This Python code snippet is used to split an audio file into chunks based on silence.\n",
        "\n",
        "The `split_on_silence` function splits the audio into chunks at points of silence that are at least 1000 milliseconds (or 1 second) long and quieter than -40 dBFS. The chunks are stored in the `audio_chunks` list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710952454226
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Chunk up the audio file \n",
        "sound_file = AudioSegment.from_mp3(podcast_audio_file)\n",
        "audio_chunks = split_on_silence(sound_file, min_silence_len=1000, silence_thresh=-40 )\n",
        "count = len(audio_chunks)\n",
        "print(\"Audio split into \" + str(count) + \" audio chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Call Whisper to transcribe audio\n",
        "This Python code snippet is used to transcribe each chunk of audio and concatenate the transcriptions.\n",
        "\n",
        "- The `whisper.load_model` method is called with `\"base\"` as the argument to load the base model of Whisper. This model is stored in the `model` variable.\n",
        "\n",
        "- We iterate over `audio_chunks` in loop where each chunk is transcribed, and the transcription is added to `transcript`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710952496776
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Call Whisper to transcribe audio\n",
        "model = whisper.load_model(\"base\")\n",
        "transcript = \"\"\n",
        "for i, chunk in enumerate(audio_chunks):\n",
        "    # If you have a long audio file, you can enable this to only run for a subset of chunks\n",
        "    if i < 10 or i > count - 10:\n",
        "        out_file = \"chunk{0}.wav\".format(i)\n",
        "        print(\"Exporting\", out_file)\n",
        "        chunk.export(out_file, format=\"wav\")\n",
        "        result = model.transcribe(out_file)\n",
        "        transcriptChunk = result[\"text\"]\n",
        "        print(transcriptChunk)\n",
        "        \n",
        "        # Append transcript in memory if you have sufficient memory\n",
        "        transcript += \" \" + transcriptChunk\n",
        "\n",
        "print(\"Transcript: \\n\")\n",
        "print(transcript)\n",
        "print(\"\\n\")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
