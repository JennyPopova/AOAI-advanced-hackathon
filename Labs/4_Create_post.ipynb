{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Overview\n",
        "In this lab we will call the GPT model API from Lang chain application for getting social media post by sending input parameters \"bio\" and \"transcript\" using ChatPromptTemplate and LLMChain.\n",
        "\n",
        "### LangChain Components-LLMChain and Prompt template\n",
        "\n",
        "**Langchain** is a Python library that allows you to create powerful applications using language models and prompts. **LLMChain** and **ChatPromptTemplate** are two of the main components of Langchain that enable you to generate natural language responses from various inputs.\n",
        "\n",
        "An **LLMChain** is an object that combines a prompt template and a language model. A **prompt template** is a string that defines the format and structure of the input and output for the language model. A language model is a neural network that can generate natural language text based on some input. An LLMChain takes a dictionary of key-value pairs as input, formats the prompt template using the values, passes the formatted string to the language model, and returns the language model output as a response. You can use different language models such as OpenAI, GPT-3, or Llama 2 with LLMChain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import python libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710960863950
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import AzureChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import (\n",
        "    PromptTemplate,\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Get podcast transcript created in Lab1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710960864415
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Open the file\n",
        "with open('transcript.txt', 'r') as f:\n",
        "    transcript = f.read()\n",
        "\n",
        "# Print the content\n",
        "print(transcript)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Get guest biography created in Lab3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710960864580
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Open the file\n",
        "with open('bio.txt', 'r') as f:\n",
        "    bio = f.read()\n",
        "\n",
        "# Print the content\n",
        "print(bio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Get Environment variables\n",
        "The provided code is importing the `load_dotenv` function from the `dotenv` module and using it to load environment variables from a `.env`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710960864704
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "if load_dotenv():\n",
        "    print(\"Found Azure OpenAI API Base Endpoint: \" + os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\n",
        "else: \n",
        "    print(\"Azure OpenAI API Base Endpoint not found. Have you configured the .env file?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710960864830
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
        "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
        "\n",
        "model =  os.getenv(\"AZURE_OPENAI_CHAT_MODEL\")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Create prompt using transcript and biography\n",
        "This Python code snippet is used to create a LinkedIn promo blurb for episodes of the podcast \"Behind the Tech\", given transcripts of the podcasts. The blurb is created from the first-person perspective of Kevin Scott, who hosts the podcast.\n",
        "\n",
        "- A `system_template` string is defined that describes the task of the AI model.\n",
        "\n",
        "- A `user_prompt` is created using the `PromptTemplate` class. This prompt includes a template for the LinkedIn post and placeholders for the podcast transcript and the guest's bio.\n",
        "\n",
        "- The `chat_prompt.format_prompt` method is called with the podcast transcript and the guest's bio as arguments to format the chat prompt. The `to_messages` method is then called to convert the formatted prompt into a list of messages.\n",
        "\n",
        "- The list of messages is stored in the `blurb_messages` variable and printed to the console."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710960864980
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "system_template=\"You are a helpful large language model that can create a LinkedIn promo blurb for episodes of the podcast Behind the Tech, when given transcripts of the podcasts.  The Behind the Tech podcast is hosted by Kevin Scott.\\n\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "\n",
        "user_prompt=PromptTemplate(\n",
        "    template=\"Create a short summary of this podcast episode that would be appropriate to post on LinkedIn to promote the podcast episode.  The post should be from the first-person perspective of Kevin Scott, who hosts the podcast.\\n\" +\n",
        "            \"Here is the transcript of the podcast episode: {transcript} \\n\" +\n",
        "            \"Here is the bio of the guest: {bio} \\n\",\n",
        "    input_variables=[\"transcript\", \"bio\"],\n",
        ")\n",
        "human_message_prompt = HumanMessagePromptTemplate(prompt=user_prompt)\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "# Get formatted messages for the chat completion\n",
        "blurb_messages = chat_prompt.format_prompt(transcript={transcript}, bio={bio}).to_messages()\n",
        "blurb_messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Send request to GPT model to create a LinkedIn post\n",
        "This Python code snippet is used to generate a social media blurb using the Azure OpenAI Service with the GPT-4 model. The blurb is generated based on the messages prepared in the previous steps.\n",
        "\n",
        "- An instance of `AzureChatOpenAI` is created, which is a wrapper for the Azure OpenAI Service.\n",
        "\n",
        "- The `AzureChatOpenAI` instance is called to sends a request to the Azure OpenAI Service to generate a social media blurb based on the `blurb_messages`.\n",
        "\n",
        "- The content of the response from the Azure OpenAI Service is stored in the `social_media_post` variable.\n",
        "\n",
        "- An instance of `LLMChain` is created with the `AzureChatOpenAI` instance, the chat prompt, and the output key as arguments. This object represents a chain of language model calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710960874743
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Make a call to Azure OpenAI Service to get a social media blurb, \n",
        "print(\"Calling GPT-4 model on Azure OpenAI Service to get a social media blurb...\\n\")\n",
        "gpt = AzureChatOpenAI(\n",
        "    openai_api_version=api_version,\n",
        "    deployment_name=model,\n",
        "    openai_api_type = \"azure\",\n",
        ")\n",
        "\n",
        "output = gpt(blurb_messages)\n",
        "social_media_post = output.content\n",
        "\n",
        "gpt4_chain = LLMChain(llm=gpt, prompt=chat_prompt, output_key=\"social_media_copy\")\n",
        "\n",
        "print(\"Social Media Copy:\")\n",
        "print(social_media_post)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Save social media post to file\n",
        "This Python code snippet is used to write the LinkedIn post into `\"social_media_post.txt\"` text file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710960874903
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Specify the file path\n",
        "file_path = \"social_media_post.txt\"\n",
        "\n",
        "# Write the content to the file\n",
        "with open(file_path, \"w\") as file:\n",
        "    file.write(social_media_post)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
