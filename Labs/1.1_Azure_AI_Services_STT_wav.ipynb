{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview: Using the Speech Service from Python\n",
    "\n",
    "This lab shows how to use the Speech Service through the Speech SDK for Python. It illustrates how the SDK can be used to recognize speech from file input.\n",
    "\n",
    "See the [accompanying article](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/index-speech-to-text) for more details.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before you get started, here's a list of prerequisites:\n",
    "\n",
    "* A subscription key for the Speech service. \n",
    "\n",
    "### Install Libraries in Terminal:\n",
    "* gstreamer:\n",
    "  ```sh\n",
    "  sudo apt install libgstreamer1.0-0 \\\n",
    "  gstreamer1.0-plugins-base \\\n",
    "  gstreamer1.0-plugins-good \\\n",
    "  gstreamer1.0-plugins-bad \\\n",
    "  gstreamer1.0-plugins-ugly\n",
    "  ```\n",
    "\n",
    "### Speech SDK Python Package\n",
    "\n",
    "**By downloading the Microsoft Cognitive Services Speech SDK, you acknowledge its license, see [Speech SDK license agreement](https://aka.ms/csspeech/license).**\n",
    "\n",
    "Speech SDK Python Package should be installed together with other libraries via `requirementx.txt`\n",
    "In case the pachage was not installed, the Cognitive Services Speech SDK Python package can be installed from [pyPI](https://pypi.org/) using this command:\n",
    "\n",
    "```sh\n",
    "pip install azure-cognitiveservices-speech\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Recognition Using the Speech SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, set up some general items. Import the Speech SDK Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load environment variables\n",
    "The provided code is importing the `load_dotenv` function from the `dotenv` module and using it to load environment variables from a `.env`\n",
    "To get access to Azure Speech Service via python SDK we need **Access Key** and **Servie Region**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "if load_dotenv():\n",
    "    print(\"Found Azure AI Services Speech Endpoint: \" + os.getenv(\"AZURE_AI_SPEECH_REGION\"))\n",
    "else: \n",
    "    print(\"Azure AI Services Speech Endpoint not found. Have you configured the .env file?\")\n",
    "    \n",
    "service_region = os.getenv(\"AZURE_AI_SPEECH_REGION\")\n",
    "speech_key = os.getenv(\"AZURE_AI_SPEECH_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Speech recognition from mp3 file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run speech regonition for audio files\n",
    "Here we run Speech Recognition and collect all the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "def recognize_from_file(filename):\n",
    "    # This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "    speech_config.speech_recognition_language=\"en-US\"\n",
    "\n",
    "    audio_config = speechsdk.audio.AudioConfig(filename=filename)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "\n",
    "    if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        print(\"Recognized: {}\".format(speech_recognition_result.text))\n",
    "        all_results.append(speech_recognition_result.text)\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\"No speech could be recognized: {}\".format(speech_recognition_result.no_match_details))\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = speech_recognition_result.cancellation_details\n",
    "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "            print(\"Did you set the speech resource key and region values?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "recognize_from_file(\"chunk0.wav\")\n",
    "recognize_from_file(\"chunk1.wav\")\n",
    "print(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to file\n",
    "This Python code snippet is used to write the transcribed text to `\"transcript_Azure_STT.txt\"` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path\n",
    "file_path = \"transcript_Azure_STT.txt\"\n",
    "\n",
    "# Write the content to the file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(\"\".join(all_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get transcription with speaker recognition\n",
    "\n",
    "The Azure Speech to Text service offers several powerful features:\n",
    "\n",
    "- **High-Quality Transcription**: Accurately transcribe spoken audio to text in over 100 languages and variants. It provides state-of-the-art speech recognition for high-quality transcription.\n",
    "\n",
    "- **Customizable Models**: Tailor the speech models to your specific needs. You can add domain-specific terminology by including specific words in your base vocabulary or even build your own custom speech-to-text models. This flexibility allows you to overcome barriers like background noise, accents, or unique vocabulary.\n",
    "\n",
    "- **Speaker Diarization**: Determine who said what and when by using speaker diarization. This feature helps differentiate speakers in the transcribed text.\n",
    "\n",
    "- **Automatic Formatting and Punctuation**: Get readable transcripts with automatic formatting and proper punctuation.\n",
    "\n",
    "For detailed documentation and tutorials, visit the [Speech to Text documentation page](https://docs.microsoft.com/azure/cognitive-services/speech-service/index-speech-to-text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code is needed if you want special processin for real-time speech recognition\n",
    "In this code snippet we are checking status of real-time recognition and collecting all pieces for further processing.\n",
    "If you use bach processing, instead of real-time, full outpur will be provided by the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_transcriber_recognition_canceled_cb(evt: speechsdk.SessionEventArgs):\n",
    "    print('Canceled event')\n",
    "\n",
    "def conversation_transcriber_session_stopped_cb(evt: speechsdk.SessionEventArgs):\n",
    "    print('SessionStopped event')\n",
    "\n",
    "def conversation_transcriber_transcribed_cb(evt: speechsdk.SpeechRecognitionEventArgs):\n",
    "    print('TRANSCRIBED:')\n",
    "    if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        print('\\tSpeaker ID={}'.format(evt.result.speaker_id))\n",
    "        print('\\tText={}'.format(evt.result.text))\n",
    "        all_results.append('\\n'+evt.result.speaker_id+': ')\n",
    "        all_results.append(evt.result.text)\n",
    "    elif evt.result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print('\\tNOMATCH: Speech could not be TRANSCRIBED: {}'.format(evt.result.no_match_details))\n",
    "\n",
    "def conversation_transcriber_session_started_cb(evt: speechsdk.SessionEventArgs):\n",
    "    print('SessionStarted event')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcribe audio file with diarization\n",
    "This Python function `recognize_from_file` performs speech recognition on an audio file using Azure's Speech SDK and transcribes a conversation from the audio file.\n",
    "\n",
    "- The function takes one argument, `filename`, which is the path to the audio file.\n",
    "- A speech configuration is created using `speechsdk.SpeechConfig` with a subscription key and service region. The speech recognition language is set to \"en-US\".\n",
    "- An audio configuration is created using `speechsdk.audio.AudioConfig` with the audio stream.\n",
    "- A conversation transcriber is created using `speechsdk.transcription.ConversationTranscriber` with the speech and audio configurations.\n",
    "\n",
    "- Several event handlers are connected to the conversation transcriber. These handlers will be called when the corresponding events are fired by the transcriber.\n",
    "- The function waits until `transcribing_stop` is `True`, checking every 0.5 seconds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_speackers_from_file(filename):\n",
    "  # Creates an audio stream format. For an example we are using MP3 compressed file here\n",
    "    #compressed_format = speechsdk.audio.AudioStreamFormat(compressed_stream_format=speechsdk.AudioStreamContainerFormat.MP3)\n",
    "    #callback = BinaryFileReaderCallback(filename=filename)\n",
    "    #stream = speechsdk.audio.PullAudioInputStream(stream_format=compressed_format, pull_stream_callback=callback)\n",
    "\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "    speech_config.speech_recognition_language=\"en-US\"\n",
    "    audio_config = speechsdk.audio.AudioConfig(filename=filename)\n",
    "\n",
    "    conversation_transcriber = speechsdk.transcription.ConversationTranscriber(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    transcribing_stop = False\n",
    "\n",
    "    def stop_cb(evt: speechsdk.SessionEventArgs):\n",
    "        #\"\"\"callback that signals to stop continuous recognition upon receiving an event `evt`\"\"\"\n",
    "        print('CLOSING on {}'.format(evt))\n",
    "        nonlocal transcribing_stop\n",
    "        transcribing_stop = True\n",
    "\n",
    "    # Connect callbacks to the events fired by the conversation transcriber\n",
    "    conversation_transcriber.transcribed.connect(conversation_transcriber_transcribed_cb)\n",
    "    conversation_transcriber.session_started.connect(conversation_transcriber_session_started_cb)\n",
    "    conversation_transcriber.session_stopped.connect(conversation_transcriber_session_stopped_cb)\n",
    "    conversation_transcriber.canceled.connect(conversation_transcriber_recognition_canceled_cb)\n",
    "    # stop transcribing on either session stopped or canceled events\n",
    "    conversation_transcriber.session_stopped.connect(stop_cb)\n",
    "    conversation_transcriber.canceled.connect(stop_cb)\n",
    "\n",
    "    conversation_transcriber.start_transcribing_async()\n",
    "\n",
    "    # Waits for completion.\n",
    "    while not transcribing_stop:\n",
    "        time.sleep(.5)\n",
    "\n",
    "    conversation_transcriber.stop_transcribing_async()\n",
    "\n",
    "# Main\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    all_results = []\n",
    "    recognize_speackers_from_file(\"chunk0.wav\")\n",
    "    recognize_speackers_from_file(\"chunk1.wav\")\n",
    "except Exception as err:\n",
    "    print(\"Encountered exception. {}\".format(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to file\n",
    "This Python code snippet is used to write the transcribed text to `\"transcript_Azure_STT_with_speaker.txt\"` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path\n",
    "file_path = \"transcript_Azure_STT_with_speaker.txt\"\n",
    "\n",
    "# Write the content to the file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(\"\".join(all_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
